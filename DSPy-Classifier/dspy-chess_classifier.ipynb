{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400734fb",
   "metadata": {},
   "source": [
    "# DSPY-based chessboard classification\n",
    "path of data: ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1a9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import base64\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from typing import Any, Dict, List, Optional\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dba55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dspy_image_to_pil(img: Any) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Convert dspy.Image (or similar) to a PIL.Image.\n",
    "    Handles:\n",
    "      - dspy.Image(url=...) where url is a data:...;base64,... string\n",
    "      - dspy.Image(url=...) where url is a local file path\n",
    "      - plain file-path strings\n",
    "      - direct PIL.Image\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) If it's already a PIL image\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "    # 2) If it has a .url attribute (dspy.Image)\n",
    "    url = getattr(img, \"url\", None)\n",
    "    if isinstance(url, str):\n",
    "        s = url.strip()\n",
    "\n",
    "        # data URL with base64 content\n",
    "        if s.startswith(\"data:\") and \"base64,\" in s:\n",
    "            b64 = s.split(\"base64,\", 1)[1]\n",
    "            data = base64.b64decode(b64)\n",
    "            return Image.open(BytesIO(data)).convert(\"RGB\")\n",
    "\n",
    "        # plain local path case (DSPy can also store raw path)\n",
    "        if os.path.exists(s):\n",
    "            return Image.open(s).convert(\"RGB\")\n",
    "\n",
    "    # 3) If it has a .path attribute (your own wrappers)\n",
    "    p = getattr(img, \"path\", None)\n",
    "    if isinstance(p, str) and os.path.exists(p):\n",
    "        return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "    # 4) If the object itself is a path-like string\n",
    "    if isinstance(img, str) and os.path.exists(img):\n",
    "        return Image.open(img).convert(\"RGB\")\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Could not convert image to PIL. \"\n",
    "        \"This is likely a dspy.Image with an unexpected internal format.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class VisionChatAdapter(dspy.ChatAdapter):\n",
    "    def format(self, signature, demos, inputs):\n",
    "        messages = super().format(signature=signature, demos=demos, inputs=inputs)\n",
    "\n",
    "        img = inputs.get(\"board_image\", None)\n",
    "        if img is None:\n",
    "            return messages\n",
    "\n",
    "        for m in reversed(messages):\n",
    "            if m.get(\"role\") != \"user\":\n",
    "                continue\n",
    "\n",
    "            # Keep existing text (whatever DSPy produced)\n",
    "            existing = m.get(\"content\", \"\")\n",
    "            if isinstance(existing, list):\n",
    "                # If DSPy already produced structured parts, just ensure an image part exists\n",
    "                has_img = any(isinstance(p, dict) and p.get(\"type\") == \"image\" for p in existing)\n",
    "                if not has_img:\n",
    "                    m[\"content\"] = [{\"type\": \"image\", \"data\": img}] + existing\n",
    "            else:\n",
    "                # Convert plain text -> structured [image, text]\n",
    "                m[\"content\"] = [\n",
    "                    {\"type\": \"image\", \"data\": img},\n",
    "                    {\"type\": \"text\", \"text\": str(existing)},\n",
    "                ]\n",
    "\n",
    "            # Keep legacy field too (harmless, helps debugging)\n",
    "            m[\"images\"] = [img]\n",
    "            break\n",
    "\n",
    "        return messages\n",
    "\n",
    "\n",
    "class LocalVisionLM(dspy.BaseLM):\n",
    "    def __init__(self, model_id: str, **kwargs):\n",
    "        super().__init__(model=model_id, model_type=\"chat\", **kwargs)\n",
    "\n",
    "        # Prefer GPU if it has enough free memory; otherwise fall back to CPU\n",
    "        self.device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                free_bytes, total_bytes = torch.cuda.mem_get_info()\n",
    "                # Require ~6GB free before loading the 4B model on GPU\n",
    "                if free_bytes > 6 * 1024**3:\n",
    "                    self.device = \"cuda\"\n",
    "            except Exception:\n",
    "                self.device = \"cpu\"\n",
    "\n",
    "        # bf16 is best on Ampere+; fallback to fp16 on older GPUs; CPU uses fp32\n",
    "        if self.device == \"cuda\":\n",
    "            self.dtype = (\n",
    "                torch.bfloat16\n",
    "                if torch.cuda.get_device_capability(0)[0] >= 8\n",
    "                else torch.float16\n",
    "            )\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        self.processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "        # Store the actual model as vision_model to avoid overwriting self.model\n",
    "        self.vision_model = AutoModelForVision2Seq.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=self.dtype,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=None,\n",
    "        )\n",
    "        self.vision_model = self.vision_model.to(self.device)\n",
    "        self.vision_model.eval()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Optional[str] = None,\n",
    "        messages: Optional[List[Dict[str, Any]]] = None,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        max_new_tokens: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Normalize generation length\n",
    "        gen_max_new = max_new_tokens if max_new_tokens is not None else (max_tokens if max_tokens is not None else 512)\n",
    "\n",
    "        # Normalize messages\n",
    "        if messages is None:\n",
    "            if prompt is None:\n",
    "                raise ValueError(\"LocalVisionLM needs either `prompt` or `messages`.\")\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        # Collect images and rebuild structured messages for the chat template\n",
    "        images: List[Image.Image] = []\n",
    "        chat_msgs: List[Dict[str, Any]] = []\n",
    "\n",
    "        for m in messages:\n",
    "            role = m.get(\"role\", \"user\")\n",
    "            content = m.get(\"content\", \"\")\n",
    "            parts = []\n",
    "\n",
    "            if isinstance(content, list):\n",
    "                for part in content:\n",
    "                    if not isinstance(part, dict):\n",
    "                        continue\n",
    "                    if part.get(\"type\") == \"image\":\n",
    "                        src = part.get(\"image\") or part.get(\"data\")\n",
    "                        if src is not None:\n",
    "                            images.append(dspy_image_to_pil(src))\n",
    "                        parts.append({\"type\": \"image\"})\n",
    "                    elif part.get(\"type\") == \"text\":\n",
    "                        parts.append({\"type\": \"text\", \"text\": str(part.get(\"text\", \"\"))})\n",
    "            else:\n",
    "                parts.append({\"type\": \"text\", \"text\": str(content)})\n",
    "\n",
    "            if not parts:\n",
    "                parts.append({\"type\": \"text\", \"text\": \"\"})\n",
    "\n",
    "            chat_msgs.append({\"role\": role, \"content\": parts})\n",
    "\n",
    "        if not images:\n",
    "            # Fallback: check legacy \"images\" field\n",
    "            for m in messages:\n",
    "                legacy_imgs = m.get(\"images\")\n",
    "                if legacy_imgs:\n",
    "                    for img in legacy_imgs:\n",
    "                        images.append(dspy_image_to_pil(img))\n",
    "\n",
    "        if not images:\n",
    "            raise ValueError(\"No image found. Did you configure VisionChatAdapter and pass board_image=?\")\n",
    "\n",
    "        # Build text prompt with image placeholders using the chat template\n",
    "        if hasattr(self.processor, \"apply_chat_template\"):\n",
    "            text = self.processor.apply_chat_template(\n",
    "                chat_msgs,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "        else:\n",
    "            text = \"\\n\".join(\n",
    "                part.get(\"text\", \"\")\n",
    "                for msg in chat_msgs\n",
    "                for part in msg.get(\"content\", [])\n",
    "                if part.get(\"type\") == \"text\"\n",
    "            ).strip()\n",
    "\n",
    "        inputs = self.processor(text=text, images=images, return_tensors=\"pt\")\n",
    "        for k, v in inputs.items():\n",
    "            if hasattr(v, \"to\"):\n",
    "                inputs[k] = v.to(self.vision_model.device)\n",
    "\n",
    "        gen_kwargs = {\"max_new_tokens\": gen_max_new}\n",
    "        if temperature is not None and temperature > 0:\n",
    "            gen_kwargs.update({\"do_sample\": True, \"temperature\": float(temperature)})\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.vision_model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "        completion = self.processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "        return [completion]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3deb6ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lotems/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "\n",
    "lm = LocalVisionLM(MODEL_ID, temperature=0.0, max_tokens=700, cache=False)\n",
    "dspy.configure(lm=lm, adapter=VisionChatAdapter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be0775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing connection\n",
    "# lm(\"Say this is a test!\", temperature=0.7)  # => ['This is a test!']\n",
    "# lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])  # => ['This is a test!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d83e9",
   "metadata": {},
   "source": [
    "## load the data\n",
    "This notebook is in ../project_home_dir/DSPy-Classifier\n",
    "The data is in ../project_home_dir/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d1cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "game7:\n",
      "  - Rows: 57\n",
      "  - Has FEN: True\n",
      "  - Has classification: False\n",
      "  - Columns: ['from_frame', 'to_frame', 'fen']\n",
      "  - Available images: 55\n",
      "\n",
      "game6:\n",
      "  - Rows: 93\n",
      "  - Has FEN: True\n",
      "  - Has classification: False\n",
      "  - Columns: ['from_frame', 'to_frame', 'fen']\n",
      "  - Available images: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70389/429323419.py:60: DeprecationWarning: Image.from_file is deprecated; use Image(file_path) instead.\n",
      "  sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n",
      "/tmp/ipykernel_70389/429323419.py:60: DeprecationWarning: Image.from_file is deprecated; use Image(file_path) instead.\n",
      "  sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "game5:\n",
      "  - Rows: 110\n",
      "  - Has FEN: True\n",
      "  - Has classification: False\n",
      "  - Columns: ['from_frame', 'to_frame', 'fen']\n",
      "  - Available images: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70389/429323419.py:60: DeprecationWarning: Image.from_file is deprecated; use Image(file_path) instead.\n",
      "  sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "game4:\n",
      "  - Rows: 186\n",
      "  - Has FEN: True\n",
      "  - Has classification: False\n",
      "  - Columns: ['from_frame', 'to_frame', 'fen']\n",
      "  - Available images: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70389/429323419.py:60: DeprecationWarning: Image.from_file is deprecated; use Image(file_path) instead.\n",
      "  sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "game2:\n",
      "  - Rows: 77\n",
      "  - Has FEN: True\n",
      "  - Has classification: False\n",
      "  - Columns: ['from_frame', 'to_frame', 'fen']\n",
      "  - Available images: 77\n",
      "\n",
      "============================================================\n",
      "Total samples: 523\n",
      "Samples with labels: 0\n",
      "Samples without labels: 523\n",
      "Samples with images: 519\n",
      "\n",
      "First few samples:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70389/429323419.py:60: DeprecationWarning: Image.from_file is deprecated; use Image(file_path) instead.\n",
      "  sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>from_frame</th>\n",
       "      <th>to_frame</th>\n",
       "      <th>fen</th>\n",
       "      <th>classification</th>\n",
       "      <th>has_label</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image</th>\n",
       "      <th>pil_image</th>\n",
       "      <th>image_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>game7</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>../data/game7_per_frame/tagged_images/frame_00...</td>\n",
       "      <td>&lt;&lt;CUSTOM-TYPE-START-IDENTIFIER&gt;&gt;[{\"type\": \"ima...</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>[[[180, 176, 167], [189, 184, 178], [191, 186,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>game7</td>\n",
       "      <td>428</td>\n",
       "      <td>428</td>\n",
       "      <td>rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>../data/game7_per_frame/tagged_images/frame_00...</td>\n",
       "      <td>&lt;&lt;CUSTOM-TYPE-START-IDENTIFIER&gt;&gt;[{\"type\": \"ima...</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>[[[185, 171, 162], [190, 176, 167], [190, 175,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game7</td>\n",
       "      <td>696</td>\n",
       "      <td>696</td>\n",
       "      <td>rnbqkb1r/pppppppp/5n2/8/3P4/8/PPP1PPPP/RNBQKBNR</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>../data/game7_per_frame/tagged_images/frame_00...</td>\n",
       "      <td>&lt;&lt;CUSTOM-TYPE-START-IDENTIFIER&gt;&gt;[{\"type\": \"ima...</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>[[[45, 44, 50], [29, 28, 36], [21, 19, 30], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>game7</td>\n",
       "      <td>708</td>\n",
       "      <td>708</td>\n",
       "      <td>rnbqkb1r/pppppppp/5n2/8/3P1B2/8/PPP1PPPP/RN1QKBNR</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>../data/game7_per_frame/tagged_images/frame_00...</td>\n",
       "      <td>&lt;&lt;CUSTOM-TYPE-START-IDENTIFIER&gt;&gt;[{\"type\": \"ima...</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>[[[28, 27, 32], [18, 17, 22], [11, 11, 19], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>game7</td>\n",
       "      <td>736</td>\n",
       "      <td>736</td>\n",
       "      <td>rnbqkb1r/ppp1pppp/5n2/3p4/3P1B2/8/PPP1PPPP/RN1...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>../data/game7_per_frame/tagged_images/frame_00...</td>\n",
       "      <td>&lt;&lt;CUSTOM-TYPE-START-IDENTIFIER&gt;&gt;[{\"type\": \"ima...</td>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>[[[33, 33, 41], [20, 20, 30], [17, 16, 30], [2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    game  from_frame  to_frame  \\\n",
       "0  game7         172       172   \n",
       "1  game7         428       428   \n",
       "2  game7         696       696   \n",
       "3  game7         708       708   \n",
       "4  game7         736       736   \n",
       "\n",
       "                                                 fen classification  \\\n",
       "0        rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR           None   \n",
       "1      rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR           None   \n",
       "2    rnbqkb1r/pppppppp/5n2/8/3P4/8/PPP1PPPP/RNBQKBNR           None   \n",
       "3  rnbqkb1r/pppppppp/5n2/8/3P1B2/8/PPP1PPPP/RN1QKBNR           None   \n",
       "4  rnbqkb1r/ppp1pppp/5n2/3p4/3P1B2/8/PPP1PPPP/RN1...           None   \n",
       "\n",
       "   has_label                                         image_path  \\\n",
       "0      False  ../data/game7_per_frame/tagged_images/frame_00...   \n",
       "1      False  ../data/game7_per_frame/tagged_images/frame_00...   \n",
       "2      False  ../data/game7_per_frame/tagged_images/frame_00...   \n",
       "3      False  ../data/game7_per_frame/tagged_images/frame_00...   \n",
       "4      False  ../data/game7_per_frame/tagged_images/frame_00...   \n",
       "\n",
       "                                               image  \\\n",
       "0  <<CUSTOM-TYPE-START-IDENTIFIER>>[{\"type\": \"ima...   \n",
       "1  <<CUSTOM-TYPE-START-IDENTIFIER>>[{\"type\": \"ima...   \n",
       "2  <<CUSTOM-TYPE-START-IDENTIFIER>>[{\"type\": \"ima...   \n",
       "3  <<CUSTOM-TYPE-START-IDENTIFIER>>[{\"type\": \"ima...   \n",
       "4  <<CUSTOM-TYPE-START-IDENTIFIER>>[{\"type\": \"ima...   \n",
       "\n",
       "                                           pil_image  \\\n",
       "0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
       "2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n",
       "\n",
       "                                         image_array  \n",
       "0  [[[180, 176, 167], [189, 184, 178], [191, 186,...  \n",
       "1  [[[185, 171, 162], [190, 176, 167], [190, 175,...  \n",
       "2  [[[45, 44, 50], [29, 28, 36], [21, 19, 30], [2...  \n",
       "3  [[[28, 27, 32], [18, 17, 22], [11, 11, 19], [1...  \n",
       "4  [[[33, 33, 41], [20, 20, 30], [17, 16, 30], [2...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data directory\n",
    "data_dir = Path(\"../data\")\n",
    "\n",
    "# Load all CSV files from different games\n",
    "games_data = {}\n",
    "all_samples = []\n",
    "\n",
    "for game_folder in data_dir.glob(\"game*_per_frame\"):\n",
    "    game_name = game_folder.name.split(\"_\")[0]  # Extract game2, game4, etc.\n",
    "    csv_path = game_folder / f\"{game_name}.csv\"\n",
    "    images_dir = game_folder / \"tagged_images\"\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Check which columns exist\n",
    "        has_fen = 'fen' in df.columns\n",
    "        has_classification = 'classification' in df.columns or 'label' in df.columns\n",
    "        \n",
    "        print(f\"\\n{game_name}:\")\n",
    "        print(f\"  - Rows: {len(df)}\")\n",
    "        print(f\"  - Has FEN: {has_fen}\")\n",
    "        print(f\"  - Has classification: {has_classification}\")\n",
    "        print(f\"  - Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Load images for each frame range\n",
    "        if images_dir.exists():\n",
    "            image_files = sorted(images_dir.glob(\"*.jpg\")) + sorted(images_dir.glob(\"*.png\"))\n",
    "            print(f\"  - Available images: {len(image_files)}\")\n",
    "            \n",
    "            # Create samples with images\n",
    "            for idx, row in df.iterrows():\n",
    "                sample = {\n",
    "                    'game': game_name,\n",
    "                    'from_frame': row.get('from_frame', None),\n",
    "                    'to_frame': row.get('to_frame', None),\n",
    "                    'fen': row.get('fen', None),\n",
    "                    'classification': row.get('classification', row.get('label', None)),\n",
    "                    'has_label': has_classification and pd.notna(row.get('classification', row.get('label', None)))\n",
    "                }\n",
    "                \n",
    "                # Try to find corresponding image\n",
    "                # Assuming images might be named with frame numbers\n",
    "                if 'from_frame' in row:\n",
    "                    frame_num = row['from_frame']\n",
    "                    matching_images = [img for img in image_files if f\"{frame_num}\" in img.name or f\"frame_{frame_num}\" in img.name]\n",
    "                    if matching_images:\n",
    "                        sample['image_path'] = str(matching_images[0])\n",
    "                        try:\n",
    "                            pil_image = Image.open(matching_images[0])\n",
    "                            image_array = np.array(pil_image)\n",
    "                            \n",
    "                            # Use the path as a simple object with path attribute\n",
    "                            class ImageWithPath:\n",
    "                                def __init__(self, path):\n",
    "                                    self.path = path\n",
    "                                def __str__(self):\n",
    "                                    return self.path\n",
    "                            \n",
    "                            sample['image'] = dspy.Image.from_file(str(matching_images[0]))\n",
    "                            sample['pil_image'] = pil_image\n",
    "                            sample['image_array'] = image_array\n",
    "                        except Exception as e:\n",
    "                            print(f\"  - Warning: Could not load image {matching_images[0]}: {e}\")\n",
    "                            sample['image'] = None\n",
    "                            sample['pil_image'] = None\n",
    "                            sample['image_array'] = None\n",
    "                \n",
    "                all_samples.append(sample)\n",
    "        \n",
    "        games_data[game_name] = df\n",
    "\n",
    "# Create comprehensive dataframe\n",
    "all_data = pd.DataFrame(all_samples)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total samples: {len(all_data)}\")\n",
    "print(f\"Samples with labels: {all_data['has_label'].sum()}\")\n",
    "print(f\"Samples without labels: {(~all_data['has_label']).sum()}\")\n",
    "print(f\"Samples with images: {all_data['image'].notna().sum()}\")\n",
    "print(f\"\\nFirst few samples:\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404e2a6",
   "metadata": {},
   "source": [
    "# Define signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cfe65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PieceClassificationSignature(dspy.Signature):\n",
    "    \"\"\"Classify chess pieces in a board square from an image.\n",
    "    \n",
    "    Args:\n",
    "        board_image: Image containing the chessboard or square\n",
    "        square_position: Board position (e.g., \"e4\", \"a1\")\n",
    "        \n",
    "    Returns:\n",
    "        piece: Chess piece notation (K/Q/R/B/N/P for white, k/q/r/b/n/p for black, . for empty, ? for unknown/occluded)\n",
    "        confidence: Confidence level of the classification (high/medium/low)\n",
    "        reasoning: Brief explanation of the classification decision\n",
    "    \"\"\"\n",
    "    board_image: dspy.Image = dspy.InputField(desc=\"Chessboard image or square region\")\n",
    "    square_position: str = dspy.InputField(desc=\"Square position in algebraic notation (e.g., e4)\")\n",
    "    piece: str = dspy.OutputField(desc=\"Piece: K/Q/R/B/N/P (white) or k/q/r/b/n/p (black) or . (empty) or ? (unknown/occluded)\")\n",
    "    confidence: str = dspy.OutputField(desc=\"Confidence: high/medium/low\")\n",
    "    reasoning: str = dspy.OutputField(desc=\"Why you made this classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4afdb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardStateSignature(dspy.Signature):\n",
    "    \"\"\"Analyze entire chessboard image and classify all 64 squares in one call.\n",
    "    \n",
    "    This signature is preferred over 64 individual square calls for efficiency.\n",
    "    Uses global board context for better accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        piece_json: JSON mapping square->piece for all squares. Use ? for unknown/occluded.\n",
    "        fen_notation: Standard FEN notation string (8 ranks), using ? for unknown squares\n",
    "        board_confidence: Overall confidence score (0.0-1.0)\n",
    "        occlusion_notes: List of squares that are occluded or unclear (e.g., \"e4, f5\")\n",
    "    \"\"\"\n",
    "    board_image: dspy.Image = dspy.InputField(desc=\"Full chessboard image\")\n",
    "    piece_json: str = dspy.OutputField(desc='JSON: {\"a1\": \"R\", \"e1\": \"K\", \"e4\": \".\", \"f5\": \"?\", ...} where ? = unknown/occluded')\n",
    "    fen_notation: str = dspy.OutputField(desc=\"FEN notation (position part only), use ? for unknown squares\")\n",
    "    board_confidence: float = dspy.OutputField(desc=\"Confidence score: 0.0-1.0\")\n",
    "    occlusion_notes: str = dspy.OutputField(desc=\"Comma-separated list of occluded squares (e.g., 'e4, f5')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0d8a6",
   "metadata": {},
   "source": [
    "Let's test the performance on the first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb854e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02f942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dspy.adapters.types.image.Image'> data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058953cc36af4d3fb758958f6cd828dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No image found. Did you configure VisionChatAdapter and pass board_image=?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:188\u001b[39m, in \u001b[36mChatAdapter.parse\u001b[39m\u001b[34m(self, signature, completion)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     fields[k] = \u001b[43mparse_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput_fields\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/utils.py:178\u001b[39m, in \u001b[36mparse_value\u001b[39m\u001b[34m(value, annotation)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTypeAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m pydantic.ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:441\u001b[39m, in \u001b[36mTypeAdapter.validate_python\u001b[39m\u001b[34m(self, object, strict, extra, from_attributes, context, experimental_allow_partial, by_alias, by_name)\u001b[39m\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    437\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    438\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    439\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperimental_allow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for float\n  Input should be a valid number [type=float_type, input_value={'': 'the value you produ...e a single float value'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/float_type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAdapterParseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:38\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/base.py:199\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m    198\u001b[39m outputs = lm(messages=inputs, **lm_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/base.py:135\u001b[39m, in \u001b[36mAdapter._call_postprocess\u001b[39m\u001b[34m(self, processed_signature, original_signature, outputs, lm)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m field_name \u001b[38;5;129;01min\u001b[39;00m original_signature.output_fields.keys():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:190\u001b[39m, in \u001b[36mChatAdapter.parse\u001b[39m\u001b[34m(self, signature, completion)\u001b[39m\n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m AdapterParseError(\n\u001b[32m    191\u001b[39m                 adapter_name=\u001b[33m\"\u001b[39m\u001b[33mChatAdapter\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m                 signature=signature,\n\u001b[32m    193\u001b[39m                 lm_response=completion,\n\u001b[32m    194\u001b[39m                 message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to parse field \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from the LM response. Error message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    195\u001b[39m             )\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fields.keys() != signature.output_fields.keys():\n",
      "\u001b[31mAdapterParseError\u001b[39m: Failed to parse field board_confidence with value {board_confidence}        # note: the value you produce must be a single float value from the LM response. Error message: 1 validation error for float\n  Input should be a valid number [type=float_type, input_value={'': 'the value you produ...e a single float value'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/float_type\n\nAdapter ChatAdapter failed to parse the LM response. \n\nLM Response: system\nYour input fields are:\n1. `board_image` (Image): Full chessboard image\nYour output fields are:\n1. `piece_json` (str): JSON: {\"a1\": \"R\", \"e1\": \"K\", \"e4\": \".\", \"f5\": \"?\", ...} where ? = unknown/occluded\n2. `fen_notation` (str): FEN notation (position part only), use ? for unknown squares\n3. `board_confidence` (float): Confidence score: 0.0-1.0\n4. `occlusion_notes` (str): Comma-separated list of occluded squares (e.g., 'e4, f5')\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## board_image ## ]]\n{board_image}\n\n[[ ## piece_json ## ]]\n{piece_json}\n\n[[ ## fen_notation ## ]]\n{fen_notation}\n\n[[ ## board_confidence ## ]]\n{board_confidence}        # note: the value you produce must be a single float value\n\n[[ ## occlusion_notes ## ]]\n{occlusion_notes}\n\n[[ ## completed ## ]]\nIn adhering to this structure, your objective is: \n        Analyze entire chessboard image and classify all 64 squares in one call.\n        \n        This signature is preferred over 64 individual square calls for efficiency.\n        Uses global board context for better accuracy.\n        \n        Returns:\n            piece_json: JSON mapping square->piece for all squares. Use ? for unknown/occluded.\n            fen_notation: Standard FEN notation string (8 ranks), using ? for unknown squares\n            board_confidence: Overall confidence score (0.0-1.0)\n            occlusion_notes: List of squares that are occluded or unclear (e.g., \"e4, f5\")\nuser\n[[ ## board_image ## ]]\n\n\nRespond with the corresponding output fields, starting with the field `[[ ## piece_json ## ]]`, then `[[ ## fen_notation ## ]]`, then `[[ ## board_confidence ## ]]` (must be formatted as a valid Python float), then `[[ ## occlusion_notes ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\nassistant\n[[ ## piece_json ## ]]\n{\"a1\": \"R\", \"b1\": \"N\", \"c1\": \"B\", \"d1\": \"K\", \"e1\": \"Q\", \"f1\": \"B\", \"g1\": \"R\", \"h1\": \"P\", \"a2\": \"P\", \"b2\": \"P\", \"c2\": \"P\", \"d2\": \"P\", \"e2\": \"P\", \"f2\": \"P\", \"g2\": \"P\", \"h2\": \"P\", \"a3\": \"P\", \"b3\": \"P\", \"c3\": \"P\", \"d3\": \"P\", \"e3\": \"P\", \"f3\": \"P\", \"g3\": \"P\", \"h3\": \"P\", \"a4\": \"P\", \"b4\": \"P\", \"c4\": \"P\", \"d4\": \"P\", \"e4\": \"P\", \"f4\": \"P\", \"g4\": \"P\", \"h4\": \"P\", \"a5\": \"P\", \"b5\": \"P\", \"c5\": \"P\", \"d5\": \"P\", \"e5\": \"P\", \"f5\": \"P\", \"g5\": \"P\", \"h5\": \"P\", \"a6\": \"P\", \"b6\": \"P\", \"c6\": \"P\", \"d6\": \"P\", \"e6\": \"P\", \"f6\": \"P\", \"g6\": \"P\", \"h6\": \"P\", \"a7\": \"P\", \"b7\": \"P\", \"c7\": \"P\", \"d7\": \"P\", \"e7\": \"P\", \"f7\": \"P\", \"g7\": \"P\", \"h7\": \"P\", \"a8\": \"P\", \"b8\": \"P\", \"c8\": \"P\", \"d8\": \"P\", \"e8\": \"P\", \"f8\": \"P\", \"g8\": \"P\", \"h8\": \"P\"}\n[[ ## fen_notation ## ]]\nrnbqkbnr/pppppppp/1p1p1p1p/2p1p1p1/2P1P1P1/2P1P1P1/PPPPPPPP/RNBQ \n\nExpected to find output fields in the LM response: [piece_json, fen_notation, board_confidence, occlusion_notes] \n\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m,\u001b[32m5\u001b[39m)):\n\u001b[32m     16\u001b[39m     example = test_examples[i]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     predictions.append(\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print predictions\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, prediction \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(predictions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/predict/predict.py:103\u001b[39m, in \u001b[36mPredict.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mself\u001b[39m._get_positional_args_error_message())\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/utils/callback.py:326\u001b[39m, in \u001b[36mwith_callbacks.<locals>.sync_wrapper\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m callbacks = _get_active_callbacks(instance)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m call_id = uuid.uuid4().hex\n\u001b[32m    330\u001b[39m _execute_start_callbacks(instance, fn, call_id, callbacks, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/primitives/module.py:81\u001b[39m, in \u001b[36mModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_lm_usage(tokens, output)\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/predict/predict.py:192\u001b[39m, in \u001b[36mPredict.forward\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m settings.context(send_stream=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         completions = \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_postprocess(completions, signature, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:47\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/json_adapter.py:73\u001b[39m, in \u001b[36mJSONAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     67\u001b[39m     lm: LM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m     72\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_json_adapter_call_common\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m     75\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/json_adapter.py:52\u001b[39m, in \u001b[36mJSONAdapter._json_adapter_call_common\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs, call_fn)\u001b[39m\n\u001b[32m     49\u001b[39m params = litellm.get_supported_openai_params(model=lm.model, custom_llm_provider=provider)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m has_tool_calls = \u001b[38;5;28many\u001b[39m(field.annotation == ToolCalls \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m signature.output_fields.values())\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Some models support json mode but not structured outputs\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Follows guidance from: https://docs.litellm.ai/docs/completion/json_mode#check-model-support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:46\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_adapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JSONAdapter\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ContextWindowExceededError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, JSONAdapter):\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# On context window exceeded error or already using JSONAdapter, we don't want to retry with a different\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# adapter.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m JSONAdapter()(lm, lm_kwargs, signature, demos, inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/chat_adapter.py:38\u001b[39m, in \u001b[36mChatAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     31\u001b[39m     lm: LM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m     36\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     40\u001b[39m         \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson_adapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JSONAdapter\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DL_Oren/CSC-BSR/.venv/lib/python3.13/site-packages/dspy/adapters/base.py:198\u001b[39m, in \u001b[36mAdapter.__call__\u001b[39m\u001b[34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[39m\n\u001b[32m    195\u001b[39m processed_signature = \u001b[38;5;28mself\u001b[39m._call_preprocess(lm, lm_kwargs, signature, inputs)\n\u001b[32m    196\u001b[39m inputs = \u001b[38;5;28mself\u001b[39m.format(processed_signature, demos, inputs)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m outputs = \u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_postprocess(processed_signature, signature, outputs, lm)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mLocalVisionLM.__call__\u001b[39m\u001b[34m(self, prompt, messages, max_tokens, max_new_tokens, temperature, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m                 images.append(dspy_image_to_pil(img))\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m images:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo image found. Did you configure VisionChatAdapter and pass board_image=?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Build text prompt with image placeholders using the chat template\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.processor, \u001b[33m\"\u001b[39m\u001b[33mapply_chat_template\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: No image found. Did you configure VisionChatAdapter and pass board_image=?"
     ]
    }
   ],
   "source": [
    "# create 10 Examples from the data, nothing fancy, just for testing and not dspy.example\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "test_examples = []\n",
    "for i, row in all_data.head(5).iterrows():\n",
    "    image = row.get('image', None)\n",
    "    if image is not None:\n",
    "        test_examples.append(image)\n",
    "img = test_examples[0]\n",
    "print(type(img), getattr(img, \"url\", None)[:60])\n",
    "# create a classifier based on the signature\n",
    "classifier = dspy.Predict(signature=BoardStateSignature)\n",
    "# run predictions\n",
    "predictions = []\n",
    "for i in tqdm(range(0,5)):\n",
    "    example = test_examples[i]\n",
    "    predictions.append(classifier(board_image=example))\n",
    "\n",
    "# print predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"\\nPrediction for Example {i+1}:\")\n",
    "    print(f\"FEN Notation: {prediction.fen_notation}\")\n",
    "    print(f\"Piece JSON: {prediction.piece_json}\")\n",
    "    print(f\"Board Confidence: {prediction.board_confidence}\")\n",
    "    print(f\"Occlusion Notes: {prediction.occlusion_notes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eac5aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for Example 1:\n",
      "FEN Notation: {fen_notation}\n",
      "Piece JSON: {piece_json}\n",
      "Board Confidence: {board_confidence}\n",
      "Occlusion Notes: {occlusion_notes}\n",
      "\n",
      "Prediction for Example 2:\n",
      "FEN Notation: {fen_notation}\n",
      "Piece JSON: {piece_json}\n",
      "Board Confidence: {board_confidence}\n",
      "Occlusion Notes: {occlusion_notes}\n",
      "\n",
      "Prediction for Example 3:\n",
      "FEN Notation: {fen_notation}\n",
      "Piece JSON: {piece_json}\n",
      "Board Confidence: {board_confidence}\n",
      "Occlusion Notes: {occlusion_notes}\n",
      "\n",
      "Prediction for Example 4:\n",
      "FEN Notation: {fen_notation}\n",
      "Piece JSON: {piece_json}\n",
      "Board Confidence: {board_confidence}\n",
      "Occlusion Notes: {occlusion_notes}\n",
      "\n",
      "Prediction for Example 5:\n",
      "FEN Notation: {fen_notation}\n",
      "Piece JSON: {piece_json}\n",
      "Board Confidence: {board_confidence}\n",
      "Occlusion Notes: {occlusion_notes}\n"
     ]
    }
   ],
   "source": [
    "# print predictions\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"\\nPrediction for Example {i+1}:\")\n",
    "    print(f\"FEN Notation: {prediction.fen_notation.strip()}\")\n",
    "    print(f\"Piece JSON: {prediction.piece_json}\")\n",
    "    print(f\"Board Confidence: {prediction.board_confidence}\")\n",
    "    print(f\"Occlusion Notes: {prediction.occlusion_notes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
